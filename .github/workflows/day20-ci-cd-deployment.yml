name: day20-artifact-deploy

on:
  push:
    branches: [ "ci/cd-github-actions" ]
  pull_request:
    branches: [ "ci/cd-github-actions" ]

env:
  BUCKET: "day20-demo-oubt"
  PREFIX: "scripts"
  AWS_REGION: "us-east-1" 

jobs:
  upload-artifacts:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Upload Glue and Lambda scripts to S3
        run: |
          set -euo pipefail
          # Map of local file paths to S3 file names
          declare -A FILES=(
            [day20/glue/day20-quality-score-card.py]=day20-quality-score-card.py
            [day20/glue/day20-export-to-redshift.py]=day20-export-to-redshift.py
            [day20/glue/day20-processed-curated.py]=day20-processed-curated.py
            [day20/glue/day20-processed-mdm.py]=day20-processed-mdm.py
            [day20/glue/day20-raw-processed-dimension-tables.py]=day20-raw-processed-dimension-tables.py
            [day20/glue/day20-raw-processed.py]=day20-raw-processed.py
            [day20/lambda/day20-lambda-validate-raw-file.py]=day20-lambda-validate-raw-file.py
          )

          for SRC in "${!FILES[@]}"; do
            DST="s3://${BUCKET}/${PREFIX}/${FILES[$SRC]}"
            # Verify file exists before uploading to prevent demo failure
            if [ -f "$SRC" ]; then
              echo "Uploading ${SRC} -> ${DST}"
              aws s3 cp "$SRC" "$DST"
            else
              echo "Warning: ${SRC} not found, skipping."
            fi
          done

      - name: Verify Upload
        run: |
          echo "Listing files in S3 target:"
          aws s3 ls "s3://${BUCKET}/${PREFIX}/"